{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final project with custom dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jems2019/CMPE-297-99-Project/blob/master/final_project_with_custom_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM26lztDUcou",
        "colab_type": "code",
        "outputId": "7c4cfb2d-ba42-4f91-f850-099461c5be90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%matplotlib inline\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import random\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "import json\n",
        "import datetime as dt\n",
        "import random \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "from scipy.stats import norm\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.test.is_gpu_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.0.0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cngJjw2Eirm",
        "colab_type": "code",
        "outputId": "fe9f887e-6690-4079-eac3-f60714ff06e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEx8OmisukKJ",
        "colab_type": "code",
        "outputId": "ac932db2-66ca-4dcd-dc1a-78bcb6efd083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# load data from google drive - this should work for all of us since we all have access to this folder\n",
        "file_path = '/content/drive/My Drive/Masters project /data/sandp500.csv'\n",
        "my_df = pd.read_csv(file_path)\n",
        "\n",
        "plt.plot(my_df['High'])\n",
        "plt.plot(my_df['Low'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0aCvlFIJRIO+UkolEA36SimVQP4f6ctR7XPjVh8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYDsycGLE7Y6",
        "colab_type": "text"
      },
      "source": [
        "## Custom Gym Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upevKA6_upd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df = pd.read_csv('./data/AAPL.csv')\n",
        "MAX_ACCOUNT_BALANCE = 100000000.0\n",
        "MAX_NUM_SHARES = 100000000.0\n",
        "\n",
        "# this should be set by the data lol\n",
        "MAX_SHARE_PRICE = 5000\n",
        "# max steps used to decay the reward for early steps in environment\n",
        "MAX_STEPS = 20000\n",
        "\n",
        "#start with $10,000\n",
        "INITIAL_ACCOUNT_BALANCE = 100000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDar-R3HE672",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# based off of https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e\n",
        "class StockTradingEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self, df):\n",
        "        super(StockTradingEnv, self).__init__()\n",
        "        # dataframe of stock timeline, the data needed to run the sim\n",
        "        self.df = df\n",
        "        # range of possible rewards at each state, used for the environment, never used by user\n",
        "        self.reward_range = (0, MAX_ACCOUNT_BALANCE)     # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
        "        # action space is 2 values [action_type, % amount]\n",
        "        # action type is the first 3. first is buy, second is sell, third is hold\n",
        "        # last item is the percentage of stocks to sell or buy\n",
        "        # so any value in action_type [0,1) is buy, [1,2) is sell, and [2,3] is hold\n",
        "        # action space low value and high value\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0]), high=np.array([1, 1, 1, 1]), dtype=np.float16)    # Prices contains the OHCL values for the last five prices\n",
        "        \n",
        "        # observation space is 2 arrays size 6\n",
        "        # first array holds stock timeline info, last 5 days of opening, high, low, closing, and volume of stock\n",
        "        # second array is info of the agent: balance, stocks held, net worth etc.\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=1, shape=(6, 6), dtype=np.float16)\n",
        "        \n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        # Reset the state of the environment to an initial state\n",
        "        # set these as the same to start\n",
        "        self.balance = INITIAL_ACCOUNT_BALANCE # available money to buy stock\n",
        "        self.net_worth = INITIAL_ACCOUNT_BALANCE # balance + stock equity value\n",
        "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE # greatest net worth\n",
        "\n",
        "        #start with no shares \n",
        "        self.shares_held = 0\n",
        "        #cost basis - original value of asset, purchase price adjusted by stock splits, dividends, and return of capital distributions\n",
        "        self.cost_basis = 0\n",
        "        self.total_shares_sold = 0\n",
        "        self.total_sales_value = 0\n",
        "\n",
        "        # Set the current step to a random point within the data frame to get different 'experiences' per reset\n",
        "        self.current_step = random.randint(5, len(self.df.loc[:, 'Open'].values)) \n",
        "        print('reset on ' + self.df.loc[self.current_step, 'Date'])\n",
        "        return self.next_observation()\n",
        "\n",
        "\n",
        "    def monteCarloSimulation(self, days):\n",
        "        t_intervals = days + 1\n",
        "        iterations = 100\n",
        "\n",
        "        # Estimate the historical log returns\n",
        "        # Using the percent change method to obtain the simple returns from the dataset\n",
        "        log_returns = np.log(1 + self.df[['Adj Close']].pct_change())\n",
        "\n",
        "        # mean\n",
        "        u = log_returns.mean()\n",
        "\n",
        "        # variance\n",
        "        var = log_returns.var()\n",
        "\n",
        "        # compute the drift component\n",
        "        # it is the best approximation of future rates of return of the stock\n",
        "        drift = u - (0.5 * var)\n",
        "\n",
        "        # standard deviation of log returns\n",
        "        stdev = log_returns.std()\n",
        "\n",
        "        # getting a randomized multidimensional array that will insert 2 args\n",
        "        x = np.random.rand(10, 2)\n",
        "\n",
        "        # include the random element within the ppf distribution to obtain the distance from \n",
        "        # the mean corresponding to each of the randomly generated probabilities\n",
        "        Z = norm.ppf(x)\n",
        "\n",
        "        # the daily returns will represent the e^r from the Brownian motion\n",
        "        daily_returns = np.exp(drift.values + stdev.values * norm.ppf(np.random.rand(t_intervals, iterations)))\n",
        "\n",
        "        # Create price list - the price list will equal the product of the price\n",
        "        # observed the previous day and the simulated daily return\n",
        "        # The first stock price used will be the last one from the dataset\n",
        "        S0 = self.df[['Adj Close']].iloc[-1]\n",
        "\n",
        "        # initiating price list for the list of prices with the expected stock prices\n",
        "        price_list = np.zeros_like(daily_returns)\n",
        "\n",
        "        # setting the entire first row of 10 elements to S0 because that would be the \n",
        "        # initial price for each of the 10 iterations we intend to generate\n",
        "        price_list[0] = S0\n",
        "\n",
        "        # generate values for the price list\n",
        "        for t in range(1, t_intervals):\n",
        "            price_list[t] = price_list[t - 1] * daily_returns[t]\n",
        "\n",
        "        # looks 'days' amount ahead for simulation\n",
        "        sim = np.mean(price_list[days])\n",
        "        return sim\n",
        "\n",
        "    \n",
        "    def next_observation(self):\n",
        "\n",
        "         # Creating simulated stock price estimate for the next day\n",
        "        self.simulated_closed_value = self.monteCarloSimulation(1)\n",
        "        # sims for the next 6 days\n",
        "        monte_carlo_closings = []\n",
        "        for i in range(1,7):\n",
        "            monte_carlo_closings.append(self.monteCarloSimulation(i)/MAX_SHARE_PRICE)\n",
        "\n",
        "        # Get the data points for the current day + last 5 days and scale to between 0-1\n",
        "        # uses opening values, high values, low values, closing values, and volume per day\n",
        "        frame = np.array([\n",
        "            self.df.loc[self.current_step-5: self.current_step,'Open'].values / MAX_SHARE_PRICE,\n",
        "            self.df.loc[self.current_step-5: self.current_step, 'High'].values / MAX_SHARE_PRICE,\n",
        "            self.df.loc[self.current_step-5: self.current_step, 'Low'].values / MAX_SHARE_PRICE,\n",
        "            self.df.loc[self.current_step-5: self.current_step, 'Close'].values / MAX_SHARE_PRICE,\n",
        "            self.df.loc[self.current_step-5: self.current_step, 'Volume'].values / MAX_NUM_SHARES,\n",
        "            monte_carlo_closings,\n",
        "        ])  \n",
        "        # observation includes past 5 day data, and user's balance and amount of shares hold etc.\n",
        "        # Append additional data and scale each value to between 0-1\n",
        "        obs = np.append(frame, [[\n",
        "            self.balance / MAX_ACCOUNT_BALANCE,\n",
        "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\n",
        "            self.shares_held / MAX_NUM_SHARES,\n",
        "            self.cost_basis / MAX_SHARE_PRICE,\n",
        "            self.total_shares_sold / MAX_NUM_SHARES,\n",
        "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n",
        "        ]], axis=0)\n",
        "\n",
        "        return obs\n",
        "    \n",
        "\n",
        "    def take_action(self, action):\n",
        "        # Set the current price to a random price within the time step\n",
        "        # this mimics day by day variance\n",
        "        # current price of the stock \n",
        "        current_price = random.uniform(\n",
        "            self.df.loc[self.current_step, \"Open\"],\n",
        "            self.df.loc[self.current_step, \"Close\"]) \n",
        "\n",
        "        # first three parts of action is type of action\n",
        "        # taking the max between action[0:2] \n",
        "        action_type = np.argmax(action[0:2])\n",
        "        # second part is the amount of stocks to sell/buy in percent (0,1) of possible total stocks you can buy at the time\n",
        "        # ex. if action[3] = .5 and buying, buy 50% of the possible total stocks you can buy with current balance\n",
        "        # ex. if action[3] = .5 and selling, sell 50% of your held stocks\n",
        "        # clip the value to be between [0,1]\n",
        "        amount = np.clip(action[3], 0, 1)  \n",
        "\n",
        "        if action_type == 0:\n",
        "            # Buy amount % of balance in shares\n",
        "            #total possible amount of shares you can buy\n",
        "            total_possible = int(self.balance / current_price)\n",
        "            # number of shares bought in step\n",
        "            shares_bought = int(total_possible * amount)\n",
        "\n",
        "            # calculate cost to buy the shares, and buy them by subtracting the cost from your balance\n",
        "            additional_cost = shares_bought * current_price\n",
        "            self.balance -= additional_cost\n",
        "            \n",
        "            #calculate cost basis  \n",
        "            prev_cost = self.cost_basis * self.shares_held\n",
        "            self.cost_basis = (prev_cost + additional_cost) / (self.shares_held + shares_bought)\n",
        "\n",
        "            # update the number of shares hold\n",
        "            self.shares_held += shares_bought  \n",
        "\n",
        "        elif action_type == 1:\n",
        "            # Sell amount % of shares held\n",
        "            shares_sold = int(self.shares_held * amount)\n",
        "            \n",
        "            # sell shares and add to balance\n",
        "            self.balance += shares_sold * current_price\n",
        "            self.shares_held -= shares_sold\n",
        "\n",
        "            # update some logging variables\n",
        "            self.total_shares_sold += shares_sold\n",
        "            self.total_sales_value += shares_sold * current_price  \n",
        "\n",
        "        # calcuate net worth, which is balance(cash) + shares\n",
        "        self.net_worth = self.balance + self.shares_held * current_price \n",
        "\n",
        "        # update max_net_worth and cost basis\n",
        "        if self.net_worth > self.max_net_worth:\n",
        "            self.max_net_worth = self.net_worth  \n",
        "        if self.shares_held == 0:\n",
        "            self.cost_basis = 0\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        # Execute one time step within the environment\n",
        "        self.take_action(action)  \n",
        "        self.current_step += 1  \n",
        "\n",
        "        # check if there are still possible days in the stock timeline\n",
        "        # if theere are no more days, reset to the begining of the timeline\n",
        "        if(self.current_step >= len(self.df.loc[:, 'Open'].values)):\n",
        "            self.current_step = 5\n",
        "        \n",
        "        # delay the reward based on the number of steps taken\n",
        "        # so early on in timeline, rewards are small  \n",
        "        # this can probably be changed to something more interesting\n",
        "        delay_modifier = (self.current_step / MAX_STEPS)\n",
        "\n",
        "        # check if you have lost more than half your starting money\n",
        "        done = self.net_worth < INITIAL_ACCOUNT_BALANCE/2\n",
        "\n",
        "        # reward for current action is the net worth with the delay\n",
        "        # reward = self.net_worth * delay_modifier\n",
        "        if not (done): # positive reward if didn't lose money\n",
        "          reward = 1 + delay_modifier\n",
        "        else:\n",
        "          reward = -10\n",
        "\n",
        "        obs = self.next_observation()  \n",
        "        return obs, reward, done, {}\n",
        "\n",
        "            \n",
        "    def render(self, mode='human', close=False):\n",
        "        # Render the environment to the screen\n",
        "        # profit is the total increase of net worth from the start\n",
        "        profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\n",
        "\n",
        "        print(f'Step: {self.current_step}')\n",
        "        print('Date: ' + str( self.df.loc[self.current_step, 'Date']))\n",
        "        print(f'Balance: {self.balance}')\n",
        "        print(\n",
        "            f'Shares held: {self.shares_held} (Total sold: {self.total_shares_sold})')\n",
        "        print(\n",
        "            f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\n",
        "        print(\n",
        "            f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\n",
        "        print(f'Profit: {profit}')\n",
        "\n",
        "    def get_profit(self):\n",
        "        return self.net_worth - INITIAL_ACCOUNT_BALANCE\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Z3P9y7SYzi",
        "colab_type": "text"
      },
      "source": [
        "# Ring Buffer for memory managment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrECyv44SYoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RingBuffer:\n",
        "    \"\"\" class that implements a not-yet-full buffer \"\"\"\n",
        "    def __init__(self,size_max):\n",
        "        self.max = size_max\n",
        "        self.data = []\n",
        "\n",
        "    class __Full:\n",
        "        \"\"\" class that implements a full buffer \"\"\"\n",
        "        def append(self, x):\n",
        "            \"\"\" Append an element overwriting the oldest one. \"\"\"\n",
        "            self.data[self.cur] = x\n",
        "            self.cur = (self.cur+1) % self.max\n",
        "        def get(self):\n",
        "            \"\"\" return list of elements in correct order \"\"\"\n",
        "            return self.data[self.cur:]+self.data[:self.cur]\n",
        "\n",
        "    def append(self,x):\n",
        "        \"\"\"append an element at the end of the buffer\"\"\"\n",
        "        self.data.append(x)\n",
        "        if len(self.data) == self.max:\n",
        "            self.cur = 0\n",
        "            # Permanently change self's class from non-full to full\n",
        "            self.__class__ = self.__Full\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\" Return a list of elements from the oldest to the newest. \"\"\"\n",
        "        return self.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lMwN3Fzu_6K",
        "colab_type": "text"
      },
      "source": [
        "# DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1JvZcH7uxbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deep Q-learning Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        \n",
        "        self.state_size = state_size # 6 x 7 - observation space\n",
        "        self.action_size = action_size # (buy, sell, hold), (percentage for action)\n",
        "        self.memory = RingBuffer(100) # holds the last 100 days\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_shape=(self.state_size,), activation='relu')) # will change to 42 once we change to 6*7 state size\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size)) # (buy, sell, hold), (percentage for action)\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        return model\n",
        "\n",
        "    # puts into memory\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # choose random action based on epsilon\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.rand(1, self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return act_values  # returns action\n",
        "        \n",
        "    # learn function - train\n",
        "    def replay(self, batch_size):\n",
        "        # batch size sample from memory\n",
        "        # ex: if batch size is 32, it would take 32 random days from the 1000 memory that we have\n",
        "        minibatch = random.sample(self.memory.get(), batch_size) \n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                # Bellman equation\n",
        "                target = reward + self.gamma * np.amax(self.model.predict(next_state.flatten().reshape(1,36))[0])\n",
        "                \n",
        "            target_f = self.model.predict(state.flatten().reshape(1,36))\n",
        "            #target_f[0][action] = target\n",
        "            self.model.fit(state.flatten().reshape(1,36), target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        del minibatch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvNK-wvcvC0O",
        "colab_type": "text"
      },
      "source": [
        "# Build Environments and Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65ZjF90fuyTj",
        "colab_type": "code",
        "outputId": "f0a7ca16-d0ee-4e9e-a59f-898088b6afbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "env = StockTradingEnv(my_df)\n",
        "\n",
        "print(env.observation_space.shape)\n",
        "print(env.action_space.shape)\n",
        "env.reset()\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "agent = DQNAgent(42, 4)\n",
        "\n",
        "action = agent.act(env.next_observation().reshape(1,42))\n",
        "print(action)\n",
        "\n",
        "obs, reward, done, _ = env.step(action[0])\n",
        "\n",
        "print(obs)\n",
        "print(reward)\n",
        "#print(env.next_observation().flatten().reshape(1,36))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reset on 2012-01-06\n",
            "(6, 6)\n",
            "(4,)\n",
            "reset on 2011-10-20\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 24)                1032      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 100       \n",
            "=================================================================\n",
            "Total params: 2,332\n",
            "Trainable params: 2,332\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[0.71179876 0.44572883 0.98825365 0.25926727]]\n",
            "[[2.41130005e-01 2.44893994e-01 2.40150000e-01 2.44691992e-01\n",
            "  2.41984009e-01 2.43078003e-01]\n",
            " [2.44921997e-01 2.44893994e-01 2.46619995e-01 2.45928003e-01\n",
            "  2.43906006e-01 2.47806006e-01]\n",
            " [2.41130005e-01 2.39710010e-01 2.38295996e-01 2.41262012e-01\n",
            "  2.39467993e-01 2.43078003e-01]\n",
            " [2.44915991e-01 2.40171997e-01 2.45076001e-01 2.41976001e-01\n",
            "  2.43078003e-01 2.47650000e-01]\n",
            " [4.11669000e+01 4.30070000e+01 4.84017000e+01 4.84639000e+01\n",
            "  4.87029000e+01 4.98077000e+01]\n",
            " [6.13927531e-01 6.13644592e-01 6.13834834e-01 6.12807799e-01\n",
            "  6.12359032e-01 6.12806713e-01]\n",
            " [7.45018277e-04 1.00000000e-03 2.10000000e-07 2.42839737e-01\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "1.02275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10IwdEIJvNPR",
        "colab_type": "text"
      },
      "source": [
        "# Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThvSpZY7u3Va",
        "colab_type": "code",
        "outputId": "bc0a1e66-37f4-41fd-8286-20db7c468214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "# Iterate the stock trading\n",
        "episodes = 100\n",
        "\n",
        "for e in range(episodes):\n",
        "    # reset state in the beginning of each game\n",
        "    state = env.reset()\n",
        "    # time_t represents each frame of the game\n",
        "    # Our goal is to keep the pole upright as long as possible until score of 500\n",
        "    # the more time_t the more score\n",
        "    # Runs through 1 year per episode \n",
        "    for time_t in range(500):\n",
        "        # turn this on if you want to render\n",
        "        # env.render()\n",
        "        # Decide action\n",
        "        action = agent.act(state.flatten().reshape(1,42))\n",
        "        next_state, reward, done, _ = env.step(action[0])\n",
        "        # Remember the previous state, action, reward, and done\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        # make next_state the new current state for the next frame.\n",
        "        state = next_state\n",
        "        # done becomes True when the game ends\n",
        "        # ex) The agent drops the pole\n",
        "        if done:\n",
        "            # print the score and break out of the loop\n",
        "            print(\"episode: {}/{}, score: {}\".format(e, episodes, time_t))\n",
        "            break\n",
        "    # train the agent with the experience of the episode\n",
        "    agent.replay(32)\n",
        "\n",
        "    if (e % 10 == 0):\n",
        "        print('episode ' + str(e))\n",
        "\n",
        "    #env.render()\n",
        "\n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reset on 2019-02-15\n",
            "episode: 0/100, score: 181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-189391858d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# train the agent with the experience of the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-bb6d26d23e10>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# Bellman equation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 42 into shape (1,36)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA06rXFDz7Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-tw1V85-71-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "# time_t represents each frame of the game\n",
        "# Our goal is to keep the pole upright as long as possible until score of 500\n",
        "# the more time_t the more score\n",
        "# Runs through 1 year per episode \n",
        "for time_t in range(260):\n",
        "    # turn this on if you want to render\n",
        "    # env.render()\n",
        "    # Decide action\n",
        "    action = agent.act(state.flatten().reshape(1,36))\n",
        "    next_state, reward, done, _ = env.step(action[0])\n",
        "    # Remember the previous state, action, reward, and done\n",
        "    agent.remember(state, action, reward, next_state, done)\n",
        "    # make next_state the new current state for the next frame.\n",
        "    state = next_state\n",
        "    # done becomes True when the game ends\n",
        "    # ex) The agent drops the pole\n",
        "    if done:\n",
        "        # print the score and break out of the loop\n",
        "        print(\"episode: {}/{}, score: {}\".format(e, episodes, time_t))\n",
        "        break\n",
        "\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}